{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purposes of this document\n",
    "\n",
    "set up tracking linear weights tools\n",
    "\n",
    "the observation fit a\n",
    "\n",
    "firing_rates = C x states +  Q\n",
    "\n",
    "we need to track the weights change in C\n",
    "as well as how the goodness of the fit using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental setup related to the questions\n",
    "\n",
    "this part should be configured to directly test the hypothesis put forward in the previous section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.365725Z",
     "start_time": "2021-04-16T22:33:46.809686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.simulation_features: get_enc_setup has a tuning_level of 1 \n",
      "\n",
      "examine the encoder weights distribution\n",
      "We expect the only the first four neurons carry information:\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijia-aw/BMi3D_my/lab_bmi3d/riglib/blackrock/cerelink.py:13: UserWarning: Unable to import cerebus library. Check if is installed if using the Blackrock NeuroPort system\n",
      "  warnings.warn(\"Unable to import cerebus library. Check if is installed if using the Blackrock NeuroPort system\")\n"
     ]
    }
   ],
   "source": [
    "from features.simulation_features import get_enc_setup\n",
    "\n",
    "#neuron set up : 'std (20 neurons)' or 'toy (4 neurons)' \n",
    "N_NEURONS, N_STATES, sim_C = get_enc_setup(sim_mode = 'std')\n",
    "\n",
    "print('examine the encoder weights distribution')\n",
    "print('We expect the only the first four neurons carry information:')\n",
    "print(sim_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.368944Z",
     "start_time": "2021-04-16T22:33:47.366679Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#encoder mean firing rate\n",
    "neuron_firing_rates  = [100, 100]\n",
    "\n",
    "#for comparision\n",
    "exp_conds = [f'enc. mean FR:{b} Hz' for b in neuron_firing_rates]\n",
    "\n",
    "\n",
    "#setting clda parameters \n",
    "##learner: collects paird data at batch_sizes\n",
    "RHO = 0.5\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "#assistor set up assist level\n",
    "assist_level = (0.05, 0.0)\n",
    "\n",
    "#learner and updater: actualy set up rho\n",
    "UPDATER_BATCH_TIME = 1\n",
    "UPDATER_HALF_LIFE = np.log(RHO)  * UPDATER_BATCH_TIME / np.log(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.381239Z",
     "start_time": "2021-04-16T22:33:47.369761Z"
    }
   },
   "outputs": [],
   "source": [
    "# CHANGE: game mechanics: generate task params\n",
    "N_TARGETS = 8\n",
    "N_TRIALS = 80\n",
    "\n",
    "NUM_EXP = len(exp_conds) # how many experiments we are running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config the experiments\n",
    "\n",
    "this section largely copyied and pasted from   \n",
    "bmi3d-sijia(branch)-bulti_in_experiemnts\n",
    "https://github.com/sijia66/brain-python-interface/blob/master/built_in_tasks/sim_task_KF.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dependant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.894420Z",
     "start_time": "2021-04-16T22:33:47.382503Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# make sure these directories are in the python path., \n",
    "from bmimultitasks import SimBMIControlMulti, SimBMICosEncKFDec, BMIControlMultiNoWindow\n",
    "from features import SaveHDF\n",
    "from features.simulation_features import get_enc_setup, SimKFDecoderRandom, SimCosineTunedEnc,SimIntentionLQRController, SimClockTick\n",
    "from features.simulation_features import SimHDF, SimTime\n",
    "\n",
    "from riglib import experiment\n",
    "\n",
    "from riglib.stereo_opengl.window import FakeWindow\n",
    "from riglib.bmi import train\n",
    "\n",
    "from weights import calc_p_values_for_spike_batches_use_intended_kin\n",
    "from weights import calc_single_batch_p_values_by_fitting_kinematics_to_spike_counts\n",
    "import weights\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "import itertools #for identical sequences\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  behaviour and task setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.898429Z",
     "start_time": "2021-04-16T22:33:47.895620Z"
    }
   },
   "outputs": [],
   "source": [
    "seq = SimBMIControlMulti.sim_target_seq_generator_multi(\n",
    "N_TARGETS, N_TRIALS)\n",
    "\n",
    "#create a second version of the tasks\n",
    "seqs = itertools.tee(seq, NUM_EXP + 1)\n",
    "target_seq = list(seqs[NUM_EXP])\n",
    "\n",
    "seqs = seqs[:NUM_EXP]\n",
    "\n",
    "\n",
    "SAVE_HDF = False\n",
    "SAVE_SIM_HDF = True #this makes the task data available as exp.task_data_hist\n",
    "DEBUG_FEATURE = False\n",
    "\n",
    "\n",
    "#base_class = SimBMIControlMulti\n",
    "base_class = BMIControlMultiNoWindow\n",
    "\n",
    "#for adding experimental features such as encoder, decoder\n",
    "feats = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder\n",
    "\n",
    "the cosine tuned encoder uses a poisson process, right\n",
    "https://en.wikipedia.org/wiki/Poisson_distribution\n",
    "so if the lambda is 1, then it's very likely "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.911237Z",
     "start_time": "2021-04-16T22:33:47.899314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__: selected SimCosineTunedEnc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ENCODER_TYPE = 'cosine_tuned_encoder'\n",
    "\n",
    "\n",
    "\n",
    "#actually multiply out the firing rates. \n",
    "sim_C_all = [sim_C * nfr for nfr in neuron_firing_rates]\n",
    "\n",
    "\n",
    "#set up intention feedbackcontroller\n",
    "#this ideally set before the encoder\n",
    "feats.append(SimIntentionLQRController)\n",
    "\n",
    "#set up the encoder\n",
    "if ENCODER_TYPE == 'cosine_tuned_encoder' :\n",
    "    feats.append(SimCosineTunedEnc)\n",
    "    print(f'{__name__}: selected SimCosineTunedEnc\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.923888Z",
     "start_time": "2021-04-16T22:33:47.911977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__: set base class \n",
      "__main__: selected SimKFDecoderRandom \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#clda on random \n",
    "DECODER_MODE = 'random' # random \n",
    "\n",
    "   #take care the decoder setup\n",
    "if DECODER_MODE == 'random':\n",
    "    feats.append(SimKFDecoderRandom)\n",
    "    print(f'{__name__}: set base class ')\n",
    "    print(f'{__name__}: selected SimKFDecoderRandom \\n')\n",
    "else: #defaul to a cosEnc and a pre-traind KF DEC\n",
    "    from features.simulation_features import SimKFDecoderSup\n",
    "    feats.append(SimKFDecoderSup)\n",
    "    print(f'{__name__}: set decoder to SimKFDecoderSup\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  clda: learner and updater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.933602Z",
     "start_time": "2021-04-16T22:33:47.924627Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LEARNER_TYPE = 'feedback' # to dumb or not dumb it is a question 'feedback'\n",
    "UPDATER_TYPE = 'smooth_batch' #none or \"smooth_batch\"\n",
    "\n",
    "\n",
    "#you know what? \n",
    "#learner only collects firing rates labeled with estimated estimates\n",
    "#we would also need to use the labeled data\n",
    "#now, we can set up a dumb/or not-dumb learner\n",
    "if LEARNER_TYPE == 'feedback':\n",
    "    from features.simulation_features import SimFeedbackLearner\n",
    "    feats.append(SimFeedbackLearner)\n",
    "else:\n",
    "    from features.simulation_features import SimDumbLearner\n",
    "    feats.append(SimDumbLearner)\n",
    "\n",
    "#to update the decoder.\n",
    "if UPDATER_TYPE == 'smooth_batch':\n",
    "    from features.simulation_features import SimSmoothBatch\n",
    "    feats.append(SimSmoothBatch)\n",
    "else: #defaut to none \n",
    "    print(f'{__name__}: need to specify an updater')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature adaptor setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.945134Z",
     "start_time": "2021-04-16T22:33:47.934548Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (feature_selection_feature.py, line 303)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/sijia-aw/BMi3D_my/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3417\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-13771187e646>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from feature_selection_feature import FeatureTransformer, TransformerBatchToFit\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/sijia-aw/BMi3D_my/operation_funny_chicken/adaptive_feature_selection/feature_selection_feature.py\"\u001b[0;36m, line \u001b[0;32m303\u001b[0m\n\u001b[0;31m    def transform_features(self)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from feature_selection_feature import FeatureTransformer, TransformerBatchToFit\n",
    "from feature_selection_feature import FeatureSelector, LassoFeatureSelector\n",
    "\n",
    "\n",
    "#pass the real time limit on clock\n",
    "feats.append(LassoFeatureSelector)\n",
    "\n",
    "feature_x_meth_arg = [\n",
    "    ('transpose', None ),\n",
    "]\n",
    "\n",
    "kwargs_feature = dict()\n",
    "kwargs_feature = {\n",
    "    'transform_x_flag':True,\n",
    "    'transform_y_flag':True,\n",
    "    'feature_x_transformer':FeatureTransformer(feature_x_meth_arg),\n",
    "    'feature_y_transformer':TransformerBatchToFit()\n",
    "}\n",
    "\n",
    "print('kwargs will be updated to a later time')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assistor setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Check) config the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.947674Z",
     "start_time": "2021-04-16T22:33:46.835Z"
    }
   },
   "outputs": [],
   "source": [
    "if DEBUG_FEATURE: \n",
    "    from features.simulation_features import DebugFeature\n",
    "    feats.append(DebugFeature)\n",
    "    \n",
    "if SAVE_HDF: feats.append(SaveHDF)\n",
    "if SAVE_SIM_HDF: feats.append(SimHDF)\n",
    "    \n",
    "    \n",
    "#pass the real time limit on clock\n",
    "feats.append(SimClockTick)\n",
    "feats.append(SimTime)\n",
    "\n",
    "\n",
    "kwargs_exps = list()\n",
    "\n",
    "for i in range(NUM_EXP):\n",
    "    d = dict()\n",
    "    d['assist_level'] = assist_level\n",
    "    d['sim_C'] = sim_C\n",
    "    d['batch_size'] = batch_size\n",
    "    \n",
    "    d['batch_time'] = UPDATER_BATCH_TIME\n",
    "    d['half_life'] = UPDATER_HALF_LIFE\n",
    "    \n",
    "    d.update(kwargs_feature)\n",
    "    \n",
    "    kwargs_exps.append(d)\n",
    "    \n",
    "    \n",
    "\n",
    "kwargs_exps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make and initalize experiment instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.948106Z",
     "start_time": "2021-04-16T22:33:46.838Z"
    }
   },
   "outputs": [],
   "source": [
    "#seed the experiment\n",
    "np.random.seed(0)\n",
    "\n",
    "#spawn the task\n",
    "Exp = experiment.make(base_class, feats=feats)\n",
    "\n",
    "exps = list()#create a list of experiment\n",
    "\n",
    "for i,s in enumerate(seqs):\n",
    "    e = Exp(s, **kwargs_exps[i])\n",
    "    exps.append(e)\n",
    "\n",
    "#run the ini\n",
    "for e in exps: e.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-experiment check: check the Kalman filter before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.948525Z",
     "start_time": "2021-04-16T22:33:46.841Z"
    }
   },
   "outputs": [],
   "source": [
    "print('we replace the encoder using the weights')\n",
    "print('assume, they are all randomly initialized get the first decoder')\n",
    "print('get a handle to the first decoder')\n",
    "first_decoder = exps[0].decoder\n",
    "target_C = first_decoder.filt.C\n",
    "    \n",
    "#replace the decoder\n",
    "for i,e in enumerate(exps):\n",
    "    weights.change_target_kalman_filter_with_a_C_mat(e.decoder.filt, target_C, debug=False)\n",
    "    \n",
    "print('we check the new decoder C matrix:')\n",
    "\n",
    "decoder_c_figure, axs = plt.subplots(nrows=1, \n",
    "                               ncols=NUM_EXP, figsize = [12,4])\n",
    "decoder_c_figure.suptitle('Decoder C matrix ')\n",
    "\n",
    "for i,e in enumerate(exps):\n",
    "    e.decoder.plot_C(ax = axs[i])\n",
    "    axs[i].set_title(exp_conds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment run: assemble into a complete loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  define the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.948984Z",
     "start_time": "2021-04-16T22:33:46.845Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#make this into a loop\n",
    "\n",
    "def run_exp_loop(exp,  **kwargs):\n",
    "        # riglib.experiment: line 597 - 601\n",
    "    #exp.next_trial = next(exp.gen)\n",
    "    # -+exp._parse_next_trial()np.arraynp.array\n",
    "\n",
    "\n",
    "    # we need to set the initial state\n",
    "    # per fsm.run:  line 138\n",
    "\n",
    "\n",
    "    # Initialize the FSM before the loop\n",
    "    exp.set_state(exp.state)\n",
    "    \n",
    "    finished_trials = exp.calc_state_occurrences('wait')\n",
    "    print(f'finished: {finished_trials}')\n",
    "\n",
    "\n",
    "    while exp.state is not None:\n",
    "\n",
    "        # exp.fsm_tick()\n",
    "\n",
    "        ### Execute commands#####\n",
    "        exp.exec_state_specific_actions(exp.state)\n",
    "\n",
    "        ###run the bmi loop #####\n",
    "        # _cycle\n",
    "\n",
    "        # bmi feature extraction, eh\n",
    "        #riglib.bmi: 1202\n",
    "        feature_data = exp.get_features()\n",
    "\n",
    "        # Determine the target_state and save to file\n",
    "        current_assist_level = exp.get_current_assist_level()\n",
    "        target_state = exp.get_target_BMI_state(exp.decoder.states)\n",
    "\n",
    "        # Determine the assistive control inputs to the Decoder\n",
    "        #update assistive control level\n",
    "        exp.update_level()\n",
    "        if np.any(current_assist_level) > 0:\n",
    "            current_state = exp.get_current_state()\n",
    "\n",
    "            if target_state.shape[1] > 1:\n",
    "                assist_kwargs = exp.assister(current_state, \n",
    "                                             target_state[:,0].reshape(-1,1), \n",
    "                                             current_assist_level, mode= exp.state)\n",
    "            else:\n",
    "                assist_kwargs = exp.assister(current_state, \n",
    "                                              target_state, \n",
    "                                              current_assist_level, \n",
    "                                              mode= exp.state)\n",
    "\n",
    "            kwargs.update(assist_kwargs)\n",
    "            \n",
    "        \n",
    "\n",
    "        # decode the new features\n",
    "        # riglib.bmi.bmiloop: line 1245\n",
    "        neural_features = feature_data[exp.extractor.feature_type]\n",
    "        \n",
    "\n",
    "\n",
    "        # call decoder.\n",
    "        #tmp = exp.call_decoder(neural_features, target_state, **kwargs)\n",
    "        neural_obs = neural_features\n",
    "        learn_flag = exp.learn_flag\n",
    "        task_state = exp.state\n",
    "\n",
    "        n_units, n_obs = neural_obs.shape\n",
    "        # If the target is specified as a 1D position, tile to match\n",
    "        # the number of dimensions as the neural features\n",
    "        if np.ndim(target_state) == 1 or (target_state.shape[1] == 1 and n_obs > 1):\n",
    "            target_state = np.tile(target_state, [1, n_obs])\n",
    "\n",
    "        decoded_states = np.zeros([exp.bmi_system.decoder.n_states, n_obs])\n",
    "        update_flag = False\n",
    "\n",
    "        for k in range(n_obs):\n",
    "            neural_obs_k = neural_obs[:, k].reshape(-1, 1)\n",
    "            target_state_k = target_state[:, k]\n",
    "\n",
    "            # NOTE: the conditional below is *only* for compatibility with older Carmena\n",
    "            # lab data collected using a different MATLAB-based system. In all python cases,\n",
    "            # the task_state should never contain NaN values.\n",
    "            if np.any(np.isnan(target_state_k)):\n",
    "                task_state = 'no_target'\n",
    "\n",
    "            #################################\n",
    "            # Decode the current observation\n",
    "            #################################\n",
    "            decodable_obs, decode = exp.bmi_system.feature_accumulator(\n",
    "                neural_obs_k)\n",
    "            if decode:  # if a new decodable observation is available from the feature accumulator\n",
    "                prev_state = exp.bmi_system.decoder.get_state()\n",
    "\n",
    "                exp.bmi_system.decoder(decodable_obs, **kwargs)\n",
    "                # Determine whether the current state or previous state should be given to the learner\n",
    "                if exp.bmi_system.learner.input_state_index == 0:\n",
    "                    learner_state = exp.bmi_system.decoder.get_state()\n",
    "                elif exp.bmi_system.learner.input_state_index == -1:\n",
    "                    learner_state = prev_state\n",
    "                else:\n",
    "                    print((\"Not implemented yet: %d\" %\n",
    "                           exp.bmi_system.learner.input_state_index))\n",
    "                    learner_state = prev_state\n",
    "\n",
    "                if learn_flag:\n",
    "                    exp.bmi_system.learner(decodable_obs.copy(), learner_state, target_state_k, exp.bmi_system.decoder.get_state(\n",
    "                    ), task_state, state_order=exp.bmi_system.decoder.ssm.state_order)\n",
    "\n",
    "            decoded_states[:, k] = exp.bmi_system.decoder.get_state()\n",
    "\n",
    "            ############################\n",
    "            # Update decoder parameters\n",
    "            ############################\n",
    "            if exp.bmi_system.learner.is_ready():\n",
    "                batch_data = exp.bmi_system.learner.get_batch()\n",
    "                batch_data['decoder'] = exp.bmi_system.decoder\n",
    "                kwargs.update(batch_data)\n",
    "                exp.bmi_system.updater(**kwargs)\n",
    "                exp.bmi_system.learner.disable()\n",
    "                \n",
    "                #measure features. \n",
    "                if isinstance(exp, FeatureSelector):\n",
    "                    exp.meausure_features_and_target(batch_data['spike_counts'],\n",
    "                                       batch_data['intended_kin'])\n",
    "                \n",
    "\n",
    "            new_params = None  # by default, no new parameters are available\n",
    "            if exp.bmi_system.has_updater:\n",
    "                new_params = copy.deepcopy(exp.bmi_system.updater.get_result())\n",
    "\n",
    "            # Update the decoder if new parameters are available\n",
    "            if not (new_params is None):\n",
    "                exp.bmi_system.decoder.update_params(\n",
    "                    new_params, **exp.bmi_system.updater.update_kwargs)\n",
    "                new_params['intended_kin'] = batch_data['intended_kin']\n",
    "                new_params['spike_counts_batch'] = batch_data['spike_counts']\n",
    "\n",
    "                exp.bmi_system.learner.enable()\n",
    "                update_flag = True\n",
    "                \n",
    "\n",
    "\n",
    "                # Save new parameters to parameter history\n",
    "                exp.bmi_system.param_hist.append(new_params)\n",
    "\n",
    "\n",
    "\n",
    "        # saved as task data\n",
    "        # return decoded_states, update_flag\n",
    "        tmp = decoded_states\n",
    "        exp.task_data['internal_decoder_state'] = tmp\n",
    "\n",
    "        # reset the plant position\n",
    "        # @riglib.bmi.BMILoop.move_plant  line:1254\n",
    "        exp.plant.drive(exp.decoder)\n",
    "\n",
    "        # check state transitions and run the FSM.\n",
    "        current_state = exp.state\n",
    "\n",
    "        # iterate over the possible events which could move the task out of the current state\n",
    "        for event in exp.status[current_state]:\n",
    "            # if the event has occurred\n",
    "            if exp.test_state_transition_event(event):\n",
    "                # execute commands to end the current state\n",
    "                exp.end_state(current_state)\n",
    "\n",
    "                # trigger the transition for the event\n",
    "                exp.trigger_event(event)\n",
    "\n",
    "                # stop searching for transition events (transition events must be\n",
    "                # mutually exclusive for this FSM to function properly)\n",
    "                break\n",
    "\n",
    "        # sort out the loop params.\n",
    "        # inc cycle count\n",
    "        exp.cycle_count += 1\n",
    "\n",
    "        # save target data as was done in manualControlTasks._cycle\n",
    "        exp.task_data['target'] = exp.target_location.copy()\n",
    "        exp.task_data['target_index'] = exp.target_index\n",
    "\n",
    "        #done in bmi:_cycle after move_plant\n",
    "        exp.task_data['loop_time'] = exp.iter_time()\n",
    "\n",
    "\n",
    "        #fb_controller data\n",
    "        exp.task_data['target_state'] = target_state\n",
    "\n",
    "        #encoder data\n",
    "        #input to this is actually extractor\n",
    "        exp.task_data['ctrl_input'] = np.reshape(exp.extractor.sim_ctrl, (1,-1))\n",
    "\n",
    "        #actually output\n",
    "        exp.task_data['spike_counts'] = feature_data['spike_counts']\n",
    "\n",
    "\n",
    "        #save the decoder_state\n",
    "        #from BMILoop.move_plant\n",
    "        exp.task_data['decoder_state'] = exp.decoder.get_state(shape=(-1,1))\n",
    "        \n",
    "        #save bmi_data\n",
    "        exp.task_data['update_bmi'] = update_flag\n",
    "\n",
    "\n",
    "        # as well as plant data.\n",
    "        plant_data = exp.plant.get_data_to_save()\n",
    "        for key in plant_data:\n",
    "            exp.task_data[key] = plant_data[key]\n",
    "\n",
    "        # clda data handled in the above call.\n",
    "\n",
    "        # save to the list hisory of data.\n",
    "        exp.task_data_hist.append(exp.task_data.copy())\n",
    "        \n",
    "        #print out the trial update whenever wait count changes, alright. \n",
    "        if finished_trials != exp.calc_state_occurrences('wait'):\n",
    "            finished_trials = exp.calc_state_occurrences('wait')\n",
    "            print(f'finished trials :{finished_trials} with a current assist level of {exp.get_current_assist_level()}')\n",
    "\n",
    "\n",
    "    if exp.verbose:\n",
    "        print(\"end of FSM.run, task state is\", exp.state)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## actually running the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.949482Z",
     "start_time": "2021-04-16T22:33:46.854Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,e in enumerate(exps):\n",
    "    run_exp_loop(e, **kwargs_exps[i])\n",
    "    print(f'Finished running  {exp_conds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-25T01:00:26.027506Z",
     "start_time": "2021-01-25T01:00:26.024320Z"
    }
   },
   "source": [
    "# Postprocessing the data for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.949873Z",
     "start_time": "2021-04-16T22:33:46.857Z"
    }
   },
   "outputs": [],
   "source": [
    "for e in  exps: print(e.calc_state_occurrences('reward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## declare defs and conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.950353Z",
     "start_time": "2021-04-16T22:33:46.861Z"
    }
   },
   "outputs": [],
   "source": [
    "FRAME_RATE = 60\n",
    "INT_WINDOW_TIME = 10 # s for looking at sample raw data\n",
    "\n",
    "# some conventions as we go down the loop\n",
    "X_VEL_STATE_IND = 3\n",
    "Y_VEL_STATE_IND = 5\n",
    "X_POS_STATE_IND = 0\n",
    "Y_POS_STATE_IND = 2\n",
    "\n",
    "state_indices = [X_POS_STATE_IND,\n",
    "                 Y_POS_STATE_IND,\n",
    "                 X_VEL_STATE_IND,\n",
    "                 Y_VEL_STATE_IND]\n",
    "state_names = ['x pos ', 'y pos', 'x vel', 'y vel']\n",
    "\n",
    "\n",
    "INT_WIN_SAMPLES = INT_WINDOW_TIME * FRAME_RATE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor out the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.950862Z",
     "start_time": "2021-04-16T22:33:46.864Z"
    }
   },
   "outputs": [],
   "source": [
    "task_data_hist_np_all = [np.array(e.task_data_hist) for e in exps]\n",
    "len(task_data_hist_np_all)\n",
    "task_data_hist_np_all[0].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.951395Z",
     "start_time": "2021-04-16T22:33:46.867Z"
    }
   },
   "outputs": [],
   "source": [
    "finished_times_in_seconds = [int(len(s)/FRAME_RATE) for s in task_data_hist_np_all]\n",
    "finished_times_in_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall  trial statistics succuss rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.951967Z",
     "start_time": "2021-04-16T22:33:46.871Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_state(state_log:list, state_to_match:str)->list:\n",
    "    '''#set axis limits of plot (x=0 to 20, y=0 to 20)\n",
    "plt.axis([0, 20, 0, 20])\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "    state_log: a list of tuples (state:string, start_time: float)\n",
    "    state_to_watch\n",
    "    \n",
    "    returns a list of element type\n",
    "    '''\n",
    "    \n",
    "    return list(filter(lambda k: k[0] == state_to_match, state_log) )\n",
    "\n",
    "def calc_inter_wait_times(wait_log: list)-> list:\n",
    "    \"\"\"\n",
    "    state_log: a list of tuples (\"wait\", start_time: float)\n",
    "    return a list of tuples: (\"wait\", start_time: float, diff_time)\n",
    "    \"\"\"\n",
    "    wait_log_with_diff = list()\n",
    "    for i, wait_state in enumerate(wait_log):\n",
    "        if i == len(wait_log)-1: #there is nothing to subtract, just put zero.\n",
    "            wait_log_with_diff.append((wait_state[1],  0))\n",
    "            \n",
    "        else:\n",
    "            finish_time = wait_log[i+1][1]\n",
    "            wait_log_with_diff.append((wait_state[1],  finish_time - wait_state[1]))\n",
    "    \n",
    "    return np.array(wait_log_with_diff[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.952797Z",
     "start_time": "2021-04-16T22:33:46.874Z"
    }
   },
   "outputs": [],
   "source": [
    "state_logs = [e.state_log for e in exps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.953267Z",
     "start_time": "2021-04-16T22:33:46.876Z"
    }
   },
   "outputs": [],
   "source": [
    "STATE_CUT_NAME =  'wait'\n",
    "#get the state logs\n",
    "wait_logs = [filter_state(s, STATE_CUT_NAME) for s in state_logs]\n",
    "\n",
    "inter_wait_times = [calc_inter_wait_times(w) for w in wait_logs]\n",
    "#this has both start times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.953700Z",
     "start_time": "2021-04-16T22:33:46.879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in inter_wait_times:\n",
    "    plt.scatter(i[:,0], i[:,1])\n",
    "\n",
    "plt.legend(exp_conds)\n",
    "plt.xlabel('Training progression(s)')\n",
    "plt.ylabel('Trial time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.954268Z",
     "start_time": "2021-04-16T22:33:46.882Z"
    }
   },
   "outputs": [],
   "source": [
    "wait_time = inter_wait_times[0]\n",
    "task_data_hist_np = task_data_hist_np_all[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.954743Z",
     "start_time": "2021-04-16T22:33:46.884Z"
    }
   },
   "outputs": [],
   "source": [
    "def sort_trials(wait_time:list, \n",
    "                target_seq:list,\n",
    "                task_data_hist_np:dict, \n",
    "                dict_keys, FRAME_RATE = 60):\n",
    "    trial_dict = list()\n",
    "    \n",
    "    for i,row in enumerate(wait_time):\n",
    "        start_time = row[0]\n",
    "        inter_wait_time = row[1]\n",
    "\n",
    "        start_sample = int(start_time * FRAME_RATE)\n",
    "        inter_wait_sample = int(inter_wait_time * FRAME_RATE)\n",
    "        stop_sample = start_sample + inter_wait_sample\n",
    "\n",
    "        single_trial_dict = dict()\n",
    "\n",
    "        for k in dict_keys:\n",
    "            \n",
    "            requested_type_data = np.squeeze(task_data_hist_np[k])\n",
    "            single_trial_dict[k] =  requested_type_data[start_sample:stop_sample,\n",
    "                                                       :]\n",
    "        #add more info\n",
    "        single_trial_dict['start_time'] = row[0]\n",
    "        single_trial_dict['inter_wait_time'] = row[1]\n",
    "        \n",
    "        #add target info\n",
    "        single_trial_dict['targets'] = target_seq[i]\n",
    "\n",
    "        #add the dictionary to the list\n",
    "        trial_dict.append(single_trial_dict)\n",
    "        \n",
    "    return trial_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.955373Z",
     "start_time": "2021-04-16T22:33:46.887Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trial_dicts_all = []\n",
    "dict_keys = ['cursor', #behaviour\n",
    "             'ctrl_input', 'spike_counts', #encoder translates intended ctrl into spike counts\n",
    "             'decoder_state']\n",
    "\n",
    "for i in range(NUM_EXP):\n",
    "    wait_time = inter_wait_times[i]\n",
    "    task_data_hist_np = task_data_hist_np_all[i]\n",
    "    \n",
    "    trial_dict_0 = sort_trials(wait_time, \n",
    "                               target_seq,\n",
    "                               task_data_hist_np, dict_keys)\n",
    "    \n",
    "    trial_dicts_all.append(trial_dict_0)\n",
    "\n",
    "len(trial_dicts_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.955897Z",
     "start_time": "2021-04-16T22:33:46.890Z"
    }
   },
   "outputs": [],
   "source": [
    "n_roi_trials = N_TRIALS - 1\n",
    "unique_targets =  np.unique(target_seq, axis = 0)\n",
    "\n",
    "\n",
    "X_CURSOR = 0\n",
    "Z_CURSOR = 2\n",
    "CIRCL_ALPHA = 0.2\n",
    "\n",
    "\n",
    "\n",
    "RANGE_LIM =  15\n",
    "figure, axes = plt.subplots() \n",
    "\n",
    "axes.set_xlim(-RANGE_LIM, RANGE_LIM)\n",
    "axes.set_ylim(-RANGE_LIM, RANGE_LIM)\n",
    "\n",
    "CIRCLE_RADIUS = exps[0].target_radius\n",
    "\n",
    "#plot the targets\n",
    "\n",
    "#plot the origin\n",
    "\n",
    "cc = plt.Circle((0,0 ), \n",
    "            radius = CIRCLE_RADIUS,\n",
    "            alpha = CIRCL_ALPHA)\n",
    "\n",
    "axes.add_artist( cc ) \n",
    "\n",
    "for origin_t in unique_targets:\n",
    "    origin = origin_t[0]\n",
    "    t = origin_t[1]\n",
    "\n",
    "    cc = plt.Circle((t[X_CURSOR],t[Z_CURSOR] ), \n",
    "                    radius = CIRCLE_RADIUS,\n",
    "                    alpha = CIRCL_ALPHA)\n",
    "                     \n",
    "    axes.set_aspect( 1 ) \n",
    "    axes.add_artist( cc ) \n",
    "    \n",
    "    \n",
    "for trial_dict in trial_dicts_all:\n",
    "    \n",
    "    sample_trial = trial_dict[n_roi_trials]\n",
    "    trial_cursor_trajectory = sample_trial['cursor']\n",
    "    \n",
    "    \n",
    "    axes.plot(trial_cursor_trajectory[:, X_CURSOR], \n",
    "             trial_cursor_trajectory[:, Z_CURSOR])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.956295Z",
     "start_time": "2021-04-16T22:33:46.892Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('finished trials:')\n",
    "\n",
    "for i,e in  enumerate(exps): \n",
    "    reward_num = e.calc_state_occurrences('reward')\n",
    "    print(f'{exp_conds[i]}: {reward_num} out of {N_TRIALS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder\n",
    "\n",
    "the job of the encoder is to directly encode intention into firing rates\n",
    "the direct measure is just pearson correlation coefficients between \n",
    "the intentions and the firing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.956661Z",
     "start_time": "2021-04-16T22:33:46.896Z"
    }
   },
   "outputs": [],
   "source": [
    "print('the encoder observation Q matrix')\n",
    "for i,e in enumerate(exps):\n",
    "    print(exp_conds[i])\n",
    "    print(e.encoder.ssm.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.957118Z",
     "start_time": "2021-04-16T22:33:46.898Z"
    }
   },
   "outputs": [],
   "source": [
    "n_exp = 0\n",
    "\n",
    "\n",
    "spike_count_sample = trial_dicts_all[n_exp][n_roi_trials]['spike_counts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.957530Z",
     "start_time": "2021-04-16T22:33:46.901Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TEXT_OFFSET_VERTICAL = -0.2\n",
    "\n",
    "decoder_c_after,axs = plt.subplots(1, NUM_EXP,\n",
    "                                  figsize = (12,4))\n",
    "\n",
    "decoder_c_after.suptitle('C matrix after')\n",
    "\n",
    "print('steady state tuning curves:')\n",
    "for i,e in enumerate(exps):\n",
    "    e.decoder.plot_C(ax = axs[i])\n",
    "    axs[i].set_title(exp_conds[i])\n",
    "    \n",
    "    #get the lower left coordinate\n",
    "    y_lim_range  = axs[i].get_ylim()[1] - axs[i].get_ylim()[0]\n",
    "    \n",
    "    axs[i].text(0, TEXT_OFFSET_VERTICAL,\n",
    "                f'finished {N_TRIALS} trials in {finished_times_in_seconds[i]} s', \n",
    "               transform = axs[i].transAxes)\n",
    "\n",
    "decoder_c_after.text(0, 1.4 * TEXT_OFFSET_VERTICAL, \n",
    "                     f'CLDA rho {RHO}',\n",
    "                    transform = axs[0].transAxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.957919Z",
     "start_time": "2021-04-16T22:33:46.904Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_c_figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare before and after the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.958381Z",
     "start_time": "2021-04-16T22:33:46.909Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_ROWS = 2 #before and after \n",
    "FIGURE_SIZE_2_by_4 = (12,4)\n",
    "figure_compare_decoder_C_matrix,axs = plt.subplots(2,NUM_EXP, \n",
    "                                                   figsize = (12,4))\n",
    "\n",
    "for i in range(NUM_EXP):\n",
    "    axs[0, i] = decoder_c_figure.get_axes()[i]\n",
    "    axs[1, i] = decoder_c_after.get_axes()[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLDA updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clda update frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.958793Z",
     "start_time": "2021-04-16T22:33:46.913Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "clda_params_all = [np.array(e.bmi_system.param_hist) for e in exps]\n",
    "\n",
    "for c in clda_params_all:\n",
    "    print(f'did clda for {len(c)} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.959161Z",
     "start_time": "2021-04-16T22:33:46.916Z"
    }
   },
   "outputs": [],
   "source": [
    "update_bmi_all = np.squeeze(task_data_hist_np_all[0]['update_bmi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.959613Z",
     "start_time": "2021-04-16T22:33:46.919Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(update_bmi_all[:240])\n",
    "plt.xlabel('frame count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reformat the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.960019Z",
     "start_time": "2021-04-16T22:33:46.923Z"
    }
   },
   "outputs": [],
   "source": [
    "clda_params = clda_params_all[1]\n",
    "\n",
    "clda_params_dict_all = list()\n",
    "\n",
    "for p in clda_params_all:\n",
    "    clda_params_dict = dict()\n",
    "    for param_key in p[0].keys():\n",
    "        clda_params_dict[param_key] = np.array([ record_i[param_key] for record_i in p])\n",
    "    \n",
    "    clda_params_dict_all.append(clda_params_dict)\n",
    "\n",
    "\n",
    "len(clda_params_dict_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## observation covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.960484Z",
     "start_time": "2021-04-16T22:33:46.926Z"
    }
   },
   "outputs": [],
   "source": [
    "n_sample = 1\n",
    "\n",
    "for i,c in enumerate(clda_params_dict_all):\n",
    "    print(exp_conds[i])\n",
    "    print(c['kf.Q'][n_sample,:,:])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clda K matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.961436Z",
     "start_time": "2021-04-16T22:33:46.930Z"
    }
   },
   "outputs": [],
   "source": [
    "kf_C = clda_params_dict['kf.C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.961922Z",
     "start_time": "2021-04-16T22:33:46.932Z"
    }
   },
   "outputs": [],
   "source": [
    "print('K matrix before:')\n",
    "print(kf_C[0,:,:])\n",
    "print('K matrix after:')\n",
    "print(kf_C[-1,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.962398Z",
     "start_time": "2021-04-16T22:33:46.935Z"
    }
   },
   "outputs": [],
   "source": [
    "N_CLDA_ROI_TIME = 800\n",
    "\n",
    "FIGURE_SIZE = (2,10)\n",
    "\n",
    "N_NEURONS\n",
    "\n",
    "f, axs = plt.subplots(1,N_NEURONS,figsize=(16,4))\n",
    "\n",
    "for i in range(N_NEURONS):\n",
    "    axs[i].plot(np.squeeze(kf_C[:N_CLDA_ROI_TIME,i, X_VEL_STATE_IND]))\n",
    "    axs[i].plot(np.squeeze(kf_C[:N_CLDA_ROI_TIME,i, Y_VEL_STATE_IND]))\n",
    "    axs[i].legend(['x vel', 'y_vel'])\n",
    "    axs[i].set_title(f'neuron {i} ')\n",
    "    axs[i].set_xlabel('clda update count ')\n",
    "    axs[i].set_ylabel('Decoder weight value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.962825Z",
     "start_time": "2021-04-16T22:33:46.938Z"
    }
   },
   "outputs": [],
   "source": [
    "exps[0].encoder.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## examine training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.963312Z",
     "start_time": "2021-04-16T22:33:46.941Z"
    }
   },
   "outputs": [],
   "source": [
    "spike_counts_batch = clda_params_dict['spike_counts_batch'] \n",
    "intended_kin = clda_params_dict['intended_kin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.963837Z",
     "start_time": "2021-04-16T22:33:46.943Z"
    }
   },
   "outputs": [],
   "source": [
    "training_sample_point = 2\n",
    "\n",
    "print('intended kinematics:')\n",
    "print(intended_kin[training_sample_point])\n",
    "\n",
    "print('spike counts:')\n",
    "print(spike_counts_batch[training_sample_point])\n",
    "\n",
    "print('trained KF C matrix:')\n",
    "print(kf_C[training_sample_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure weights change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.964251Z",
     "start_time": "2021-04-16T22:33:46.947Z"
    }
   },
   "outputs": [],
   "source": [
    "from weights_linear_regression import *\n",
    "from afs_plotting import plot_prefered_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.964724Z",
     "start_time": "2021-04-16T22:33:46.949Z"
    }
   },
   "outputs": [],
   "source": [
    "kf_C = clda_params_dict['kf.C']\n",
    "kf_C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.965238Z",
     "start_time": "2021-04-16T22:33:46.951Z"
    }
   },
   "outputs": [],
   "source": [
    "history_of_L2_norms = calc_a_history_of_matrix_L2norms_along_first_axis(kf_C,\n",
    "                                                                       (X_VEL_STATE_IND, Y_VEL_STATE_IND))\n",
    "\n",
    "history_of_L2_norms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.965651Z",
     "start_time": "2021-04-16T22:33:46.954Z"
    }
   },
   "outputs": [],
   "source": [
    "figure_compare_kf, axes_compare_kf = plt.subplots(1,3, figsize = (12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.966073Z",
     "start_time": "2021-04-16T22:33:46.956Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_of_interest = 0\n",
    "kf_slice_start = np.squeeze(kf_C[time_of_interest, :,:])\n",
    "print(f'kf_slice has kf_slice has shape {kf_slice_start.shape}')\n",
    "\n",
    "kf_slice_end = np.squeeze(kf_C[-1, :,:])\n",
    "\n",
    "\n",
    "#plot the trajectjory\n",
    "axes_compare_kf[0].plot(history_of_L2_norms)\n",
    "\n",
    "#plot the begining \n",
    "plot_prefered_directions(kf_slice_start, \n",
    "                         ax = axes_compare_kf[1])\n",
    "axes_compare_kf[0].set_title('Prefered direction vector L2 norm')\n",
    "axes_compare_kf[0].set_xlabel('update batch number')\n",
    "\n",
    "\n",
    "\n",
    "#plot the end\n",
    "plot_prefered_directions(kf_slice_end, \n",
    "                         ax = axes_compare_kf[2])\n",
    "axes_compare_kf[2].set_title('Prefered directions at conclusion')\n",
    "\n",
    "\n",
    "figure_compare_kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at lasso weights feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.966513Z",
     "start_time": "2021-04-16T22:33:46.959Z"
    }
   },
   "outputs": [],
   "source": [
    "e = exps[0]\n",
    "\n",
    "x = e.get_history_of_weights()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.966950Z",
     "start_time": "2021-04-16T22:33:46.961Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(x[:,0,:]))\n",
    "plt.xlabel('Update Time')\n",
    "plt.ylabel('Weights')\n",
    "plt.title('X Velocity Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.967384Z",
     "start_time": "2021-04-16T22:33:46.964Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(x[:,1,:]))\n",
    "plt.xlabel('Update Time')\n",
    "plt.ylabel('Weights')\n",
    "plt.title('Y Velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure ssms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.967837Z",
     "start_time": "2021-04-16T22:33:46.967Z"
    }
   },
   "outputs": [],
   "source": [
    "spike_counts_batch = clda_params_dict['spike_counts_batch'] \n",
    "intended_kin = clda_params_dict['intended_kin']\n",
    "\n",
    "print(f'spike_counts_batch has the shape of {spike_counts_batch.shape}')\n",
    "print(f'intended_kin has the shape of {intended_kin.shape}')\n",
    "\n",
    "print(f'kf_C has the shape of {kf_C.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.968277Z",
     "start_time": "2021-04-16T22:33:46.969Z"
    }
   },
   "outputs": [],
   "source": [
    "VEL_DECODING_STATES = (X_VEL_STATE_IND, Y_VEL_STATE_IND,)\n",
    "print(f'we will only be looking the indices of the x vel and y vel {VEL_DECODING_STATES}')\n",
    "\n",
    "time_slice_of_interest = 0\n",
    "print(f'time slice of interest {time_slice_of_interest}')\n",
    "\n",
    "spike_count_slice = np.squeeze(spike_counts_batch[time_slice_of_interest, :, :])\n",
    "intended_kin_slice = np.squeeze(intended_kin[time_slice_of_interest, VEL_DECODING_STATES, :])\n",
    "kf_C_slice = np.squeeze(kf_C[time_slice_of_interest, :,VEL_DECODING_STATES])\n",
    "\n",
    "#rotate the dimensions=\n",
    "spike_count_slice = spike_count_slice.T\n",
    "intended_kin_slice = intended_kin_slice.T\n",
    "kf_C_slice = kf_C_slice.T\n",
    "\n",
    "\n",
    "print(f'spike_count_slice has the shape of {spike_count_slice.shape}')\n",
    "print(f'intended_kin_slice has the shape of {intended_kin_slice.shape}')\n",
    "\n",
    "print(f'kf_C_slice has the shape of {kf_C_slice.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.968862Z",
     "start_time": "2021-04-16T22:33:46.972Z"
    }
   },
   "outputs": [],
   "source": [
    "from weights_linear_regression import calc_average_ssm_for_each_X_column\n",
    "\n",
    "ssm_slice = calc_average_ssm_for_each_X_column(intended_kin_slice, spike_count_slice, kf_C_slice)\n",
    "\n",
    "ssm_slice.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encapsulate into the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.969286Z",
     "start_time": "2021-04-16T22:33:46.974Z"
    }
   },
   "outputs": [],
   "source": [
    "(N_BATCHES, N_NEURONS, N_POINTS_IN_A_BATCH)  =  spike_counts_batch.shape\n",
    "\n",
    "#initialize an nan array\n",
    "ssm_all_batches = np.empty((N_BATCHES, N_NEURONS))\n",
    "ssm_all_batches[:] = np.nan\n",
    "\n",
    "for n_batch in range(N_BATCHES):\n",
    "    spike_count_slice = np.squeeze(spike_counts_batch[n_batch, :, :])\n",
    "    intended_kin_slice = np.squeeze(intended_kin[n_batch, VEL_DECODING_STATES, :])\n",
    "    kf_C_slice = np.squeeze(kf_C[n_batch,:,VEL_DECODING_STATES])\n",
    "    \n",
    "    #rotate the dimensions=\n",
    "    spike_count_slice = spike_count_slice.T\n",
    "    intended_kin_slice = intended_kin_slice.T\n",
    "    kf_C_slice = kf_C_slice.T\n",
    "\n",
    "    \n",
    "    ssm_slice = calc_average_ssm_for_each_X_column(intended_kin_slice, \n",
    "                                                   spike_count_slice, \n",
    "                                                   kf_C_slice)\n",
    "    ssm_all_batches[n_batch, :] = ssm_slice\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.969701Z",
     "start_time": "2021-04-16T22:33:46.978Z"
    }
   },
   "outputs": [],
   "source": [
    "neurons_of_interest = range(8)\n",
    "\n",
    "print(f'look at these two neurons {neurons_of_interest}')\n",
    "\n",
    "plt.plot(ssm_all_batches[:, neurons_of_interest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## over all ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.970136Z",
     "start_time": "2021-04-16T22:33:46.981Z"
    }
   },
   "outputs": [],
   "source": [
    "(N_BATCHES, N_NEURONS, N_POINTS_IN_A_BATCH)  =  spike_counts_batch.shape\n",
    "\n",
    "#initialize an nan array\n",
    "ssm_all_batches = np.empty((N_BATCHES,))\n",
    "\n",
    "\n",
    "for n_batch in range(N_BATCHES):\n",
    "    spike_count_slice = np.squeeze(spike_counts_batch[n_batch, :, :])\n",
    "    intended_kin_slice = np.squeeze(intended_kin[n_batch, VEL_DECODING_STATES, :])\n",
    "    kf_C_slice = np.squeeze(kf_C[n_batch, :,VEL_DECODING_STATES])\n",
    "    \n",
    "    #rotate the dimensions=\n",
    "    spike_count_slice = spike_count_slice.T\n",
    "    intended_kin_slice = intended_kin_slice.T\n",
    "    kf_C_slice = kf_C_slice.T\n",
    "    \n",
    "    ssm_slice = calc_ssm_y_minus_x_beta(intended_kin_slice, \n",
    "                                                   spike_count_slice, \n",
    "                                                   kf_C_slice)\n",
    "    ssm_all_batches[n_batch] = ssm_slice\n",
    "    \n",
    "ssm_all_batches = ssm_all_batches / N_POINTS_IN_A_BATCH\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.970512Z",
     "start_time": "2021-04-16T22:33:46.983Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ssm_all_batches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the batch time is small\n",
    "a batch should include at least 4 reaches.\n",
    "very linear. all of a sudden, it gets way.\n",
    "\n",
    "inside target, setting the int to zero. \n",
    "the hold vel is zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use variance to drop constant value features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.970890Z",
     "start_time": "2021-04-16T22:33:46.987Z"
    }
   },
   "outputs": [],
   "source": [
    "spike_counts_batch = clda_params_dict['spike_counts_batch'] \n",
    "intended_kin = clda_params_dict['intended_kin']\n",
    "\n",
    "print(f'spike_counts_batch has the shape of {spike_counts_batch.shape}')\n",
    "print(f'intended_kin has the shape of {intended_kin.shape}')\n",
    "\n",
    "batch_of_interest = 23\n",
    "print(f'only fit to 1 batch of interest: {batch_of_interest}')\n",
    "\n",
    "spike_counts_batch_sample_batch = spike_counts_batch[batch_of_interest,:,:]\n",
    "intended_kin_sample = intended_kin[batch_of_interest,:,:]\n",
    "print(f'spike counts batch sample batch is {spike_counts_batch_sample_batch.shape}')\n",
    "print(f'intended kin sample  is {intended_kin_sample.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.971259Z",
     "start_time": "2021-04-16T22:33:46.989Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "print('need to transpose to time by features')\n",
    "feature_vector = spike_counts_batch_sample_batch.T\n",
    "target_vector = intended_kin_sample.T\n",
    "print(f'the feature vector has the shape of {feature_vector.shape}')\n",
    "print(f'the target vector has the shape of {target_vector.shape}\\n')\n",
    "\n",
    "\n",
    "#set up the selector\n",
    "variance_selector = VarianceThreshold(threshold = 0)\n",
    "\n",
    "#calculate the empirical variances\n",
    "feature_variances = variance_selector.fit(feature_vector)\n",
    "\n",
    "variance_selector_params = variance_selector.get_params()\n",
    "\n",
    "#fit the transform\n",
    "transformed_feature_vec = variance_selector.fit_transform(feature_vector)\n",
    "\n",
    "#this needs to be fitted before returning the selected features, eh. \n",
    "selected_feature_indx = variance_selector.get_support()\n",
    "\n",
    "\n",
    "print(f'we know the feature variances to be {feature_variances}')\n",
    "print(f'we know the parameters are {variance_selector_params}')\n",
    "print(f'after transform, we know the dim of the feature vector {transformed_feature_vec.shape}\\n')\n",
    "print(f'we therefore, know the selected indices are {selected_feature_indx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.971694Z",
     "start_time": "2021-04-16T22:33:46.992Z"
    }
   },
   "outputs": [],
   "source": [
    "VERTICAL_FIGURE_SIZE = 4\n",
    "\n",
    "figure_feature_example, axes_feature_example = plt.subplots(1,2, \n",
    "                                                           figsize = (VERTICAL_FIGURE_SIZE * 2, VERTICAL_FIGURE_SIZE))\n",
    "\n",
    "axes_feature_example[0].plot(feature_vector)\n",
    "axes_feature_example[0].set_xlabel('sample count')\n",
    "axes_feature_example[0].set_title('Before variance selection')\n",
    "\n",
    "axes_feature_example[1].plot(transformed_feature_vec)\n",
    "axes_feature_example[1].set_xlabel('sample count')\n",
    "axes_feature_example[1].set_title('After variance selection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess lasso fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.972069Z",
     "start_time": "2021-04-16T22:33:46.994Z"
    }
   },
   "outputs": [],
   "source": [
    "spike_counts_batch = clda_params_dict['spike_counts_batch'] \n",
    "intended_kin = clda_params_dict['intended_kin']\n",
    "\n",
    "print(f'spike_counts_batch has the shape of {spike_counts_batch.shape}')\n",
    "print(f'intended_kin has the shape of {intended_kin.shape}')\n",
    "\n",
    "print(f'kf_C has the shape of {kf_C.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.972501Z",
     "start_time": "2021-04-16T22:33:46.997Z"
    }
   },
   "outputs": [],
   "source": [
    "VEL_DECODING_STATES = (X_VEL_STATE_IND, Y_VEL_STATE_IND,)\n",
    "print(f'we will only be looking the indices of the x vel and y vel {VEL_DECODING_STATES}')\n",
    "\n",
    "time_slice_of_interest = 0\n",
    "print(f'time slice of interest {time_slice_of_interest}')\n",
    "\n",
    "spike_count_slice = np.squeeze(spike_counts_batch[time_slice_of_interest, :, :])\n",
    "intended_kin_slice = np.squeeze(intended_kin[time_slice_of_interest, VEL_DECODING_STATES, :])\n",
    "kf_C_slice = np.squeeze(kf_C[time_slice_of_interest, :,VEL_DECODING_STATES])\n",
    "\n",
    "#rotate the dimensions=\n",
    "spike_count_slice = spike_count_slice.T\n",
    "intended_kin_slice = intended_kin_slice.T\n",
    "kf_C_slice = kf_C_slice.T\n",
    "\n",
    "\n",
    "print(f'spike_count_slice has the shape of {spike_count_slice.shape}')\n",
    "print(f'intended_kin_slice has the shape of {intended_kin_slice.shape}')\n",
    "\n",
    "print(f'kf_C_slice has the shape of {kf_C_slice.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.972912Z",
     "start_time": "2021-04-16T22:33:46.999Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf_C_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this matrix differs from below\n",
    "because this is mixed from the intial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.973330Z",
     "start_time": "2021-04-16T22:33:47.002Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "linear_reg_model = linear_model.LinearRegression()\n",
    "linear_reg_model.fit(intended_kin_slice, \n",
    "        spike_count_slice)\n",
    "\n",
    "linear_reg_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit to Lasso path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.973857Z",
     "start_time": "2021-04-16T22:33:47.005Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = linear_model.Lasso(alpha = 1,\n",
    "    max_iter = 100000)\n",
    "clf.fit(spike_count_slice, intended_kin_slice)\n",
    "\n",
    "print(clf.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apparently, we have this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.974754Z",
     "start_time": "2021-04-16T22:33:47.008Z"
    }
   },
   "outputs": [],
   "source": [
    "(N_BATCHES, N_NEURONS, N_POINTS_IN_A_BATCH)  =  spike_counts_batch.shape\n",
    "lasso_alpha = 1\n",
    "\n",
    "#initialize an nan array\n",
    "lasso_coefs = np.empty((N_BATCHES, len(VEL_DECODING_STATES), N_NEURONS))\n",
    "lasso_coefs[:] = np.nan\n",
    "\n",
    "for n_slice in range(N_BATCHES):\n",
    "    #get the exact dimensions\n",
    "    spike_count_slice = np.squeeze(spike_counts_batch[n_slice, :, :])\n",
    "    intended_kin_slice = np.squeeze(intended_kin[n_slice, VEL_DECODING_STATES, :])\n",
    "    \n",
    "    #rotate the dimensions=\n",
    "    spike_count_slice = spike_count_slice.T\n",
    "    intended_kin_slice = intended_kin_slice.T\n",
    "\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha = lasso_alpha,\n",
    "        max_iter = 100000)\n",
    "    clf.fit(spike_count_slice, intended_kin_slice)\n",
    "\n",
    "\n",
    "    lasso_coefs[n_slice,:,:] = clf.coef_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.975173Z",
     "start_time": "2021-04-16T22:33:47.011Z"
    }
   },
   "outputs": [],
   "source": [
    "print('to use the weight function, we need to swap axis')\n",
    "OLD_NEURON_AXIS = 2\n",
    "OLD_STATE_AXIS = 1\n",
    "\n",
    "lasso_coefs = np.swapaxes(lasso_coefs, OLD_NEURON_AXIS, OLD_STATE_AXIS)\n",
    "print(f'the new lasso coef has axis of {lasso_coefs.shape}')\n",
    "\n",
    "lasso_coefs_over_batches = calc_a_history_of_matrix_L2norms_along_first_axis(lasso_coefs,\n",
    "                                                                       (0,1))\n",
    "\n",
    "\n",
    "lasso_coefs_over_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.975550Z",
     "start_time": "2021-04-16T22:33:47.013Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure_lasso, lasso_axes = plt.subplots(1,2)\n",
    "\n",
    "encoding_neurons_of_interest = range(4)\n",
    "nonencoding_neurons_of_interest = range(4, 20)\n",
    "\n",
    "lasso_axes[0].plot(lasso_coefs_over_batches[:, encoding_neurons_of_interest])\n",
    "\n",
    "lasso_axes[1].plot(lasso_coefs_over_batches[:,nonencoding_neurons_of_interest])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.975951Z",
     "start_time": "2021-04-16T22:33:47.016Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure_lasso_directions, lasso_pds_axes = plt.subplots(1,2)\n",
    "\n",
    "\n",
    "time_of_interest = 0\n",
    "lasso_slice_start = np.squeeze(lasso_coefs[time_of_interest, :,:])\n",
    "print(f'kf_slice has kf_slice has shape {lasso_slice_start.shape}')\n",
    "\n",
    "lasso_slice_end = np.squeeze(lasso_coefs[-1, :,:])\n",
    "\n",
    "\n",
    "#plot the begining \n",
    "plot_prefered_directions(lasso_slice_start, \n",
    "                         ax = lasso_pds_axes[0])\n",
    "lasso_pds_axes[0].set_title('Prefered direction vector L2 norm at the begining')\n",
    "lasso_pds_axes[0].set_xlabel('update batch number')\n",
    "\n",
    "\n",
    "\n",
    "#plot the end\n",
    "#plot the begining \n",
    "plot_prefered_directions(lasso_slice_end, \n",
    "                         ax = lasso_pds_axes[1])\n",
    "lasso_pds_axes[1].set_title('Prefered direction vector L2 norm at the last update')\n",
    "lasso_pds_axes[1].set_xlabel('update batch number')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.976359Z",
     "start_time": "2021-04-16T22:33:47.019Z"
    }
   },
   "outputs": [],
   "source": [
    "print('get a slice of data')\n",
    "n_slice = 0\n",
    "\n",
    "#get the exact dimensions\n",
    "spike_count_slice = np.squeeze(spike_counts_batch[n_slice, :, :])\n",
    "intended_kin_slice = np.squeeze(intended_kin[n_slice, VEL_DECODING_STATES, :])\n",
    "\n",
    "#rotate the dimensions=\n",
    "spike_count_slice = spike_count_slice.T\n",
    "intended_kin_slice = intended_kin_slice.T\n",
    "\n",
    "print(f'just to double check the shapes \\\n",
    "      spike_count_slice has a shape of {spike_count_slice.shape}\\\n",
    "      and intended_kin_slice has the shape of {intended_kin_slice.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.976770Z",
     "start_time": "2021-04-16T22:33:47.022Z"
    }
   },
   "outputs": [],
   "source": [
    "eps = 1e-4 # this is the alpha range that we are interested in, right. \n",
    "CV = 5 \n",
    "n_alphas = 100\n",
    "print(f'to work with lasso regression, we are using an alpha range of {eps} as defined by alpha_min / alpha_max')\n",
    "print(f'and we are doing {CV} fold validation')\n",
    "print(f'and by default, we are using {n_alphas} alphas ')\n",
    "\n",
    "lassoCV_result = linear_model.MultiTaskLassoCV(eps = eps,\n",
    "                                     cv = CV).fit(spike_count_slice, intended_kin_slice)\n",
    "\n",
    "print(f'lassoCV_result returns the best alpha of {lassoCV_result.alpha_}')\n",
    "print(f'and the alphas we used are {lassoCV_result.alphas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the lasso path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.977191Z",
     "start_time": "2021-04-16T22:33:47.026Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "#determine the error tolerance\n",
    "eps = 3e-5\n",
    "\n",
    "alpha_lasso, coefs_lasso,_  = lasso_path(intended_kin_slice,\n",
    "                                     spike_count_slice)\n",
    "\n",
    "\n",
    "print(f'alphas have the shape of {alpha_lasso.shape}')\n",
    "print(f'coefs have the shape of {coefs_lasso.shape}')\n",
    "\n",
    "coef_dim_1, coef_2, coef_3 = coefs_lasso.shape\n",
    "\n",
    "\n",
    "reshaped_coefs_lasso = np.reshape(\n",
    "    coefs_lasso, (coef_dim_1 * coef_2, coef_3), order = 'F' #Fortan like column first like reshaping\n",
    ")\n",
    "print('transpose for plotting')\n",
    "reshaped_coefs_lasso = reshaped_coefs_lasso.T\n",
    "print(f'reshaped coefs array has dimension of {reshaped_coefs_lasso.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.977642Z",
     "start_time": "2021-04-16T22:33:47.029Z"
    }
   },
   "outputs": [],
   "source": [
    "print('let plot the coefficient trajectory')\n",
    "\n",
    "neg_log_alpha_lasso = - np.log(alpha_lasso)\n",
    "\n",
    "plt.plot(neg_log_alpha_lasso, reshaped_coefs_lasso)\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.978074Z",
     "start_time": "2021-04-16T22:33:47.032Z"
    }
   },
   "outputs": [],
   "source": [
    "print('reorient the array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.978515Z",
     "start_time": "2021-04-16T22:33:47.035Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "    [[1,2],[0,1]],\n",
    "    [[1,2],[1,2]],\n",
    "    [[3,4],[5,6]]\n",
    "])\n",
    "print(a.shape)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.978888Z",
     "start_time": "2021-04-16T22:33:47.039Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.ravel(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.979248Z",
     "start_time": "2021-04-16T22:33:47.042Z"
    }
   },
   "outputs": [],
   "source": [
    "np.reshape(a, (6,2), order = 'F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit lasso to accumulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.979614Z",
     "start_time": "2021-04-16T22:33:47.045Z"
    }
   },
   "outputs": [],
   "source": [
    "(N_BATCHES, N_NEURONS, N_POINTS_IN_A_BATCH)  =  spike_counts_batch.shape\n",
    "(N_BATCHES, N_STATES, N_POINTS_IN_A_BATCH) = intended_kin.shape\n",
    "\n",
    "print(f'{intended_kin.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.979994Z",
     "start_time": "2021-04-16T22:33:47.048Z"
    }
   },
   "outputs": [],
   "source": [
    "print('we are gonna swap axes for the state as well as ')\n",
    "OLD_NEURON_AXIS = OLD_STATE_AXIS = 1\n",
    "OLD_BATCH_TIME_AXIS =  2\n",
    "\n",
    "spike_counts_batch_reshape = np.swapaxes(spike_counts_batch, OLD_BATCH_TIME_AXIS,OLD_NEURON_AXIS)\n",
    "intended_kin_reshape = np.swapaxes(intended_kin, OLD_BATCH_TIME_AXIS,OLD_STATE_AXIS)\n",
    "\n",
    "\n",
    "print(spike_counts_batch_reshape.shape)\n",
    "print(f'the new intended kin has batch time axis as {intended_kin_reshape.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.980403Z",
     "start_time": "2021-04-16T22:33:47.051Z"
    }
   },
   "outputs": [],
   "source": [
    "N_DECODING_STATES = len(VEL_DECODING_STATES)\n",
    "\n",
    "\n",
    "lasso_alpha_all = [1, 10, 100]\n",
    "\n",
    "\n",
    "lasso_coefs_all = list()\n",
    "ols_coefs_all = list()\n",
    "\n",
    "for lasso_alpha in lasso_alpha_all:\n",
    "    #initialize an nan array\n",
    "    lasso_coefs_accumulate = np.empty((N_BATCHES,N_DECODING_STATES, N_NEURONS))\n",
    "    lasso_coefs_accumulate[:] = np.nan\n",
    "\n",
    "    #initialize \n",
    "    ols_coefs_accumulate = np.empty((N_BATCHES,N_DECODING_STATES, N_NEURONS))\n",
    "    ols_coefs_accumulate[:] = np.nan\n",
    "    \n",
    "\n",
    "    for n_batch in range(1,N_BATCHES):\n",
    "\n",
    "\n",
    "        spike_count_reshape_slice = np.reshape(spike_counts_batch_reshape[:n_batch, :, :], \n",
    "                                               (-1, N_NEURONS)) #keep the neurons and layout the batches\n",
    "\n",
    "        intended_kin_reshape_slice = np.reshape(intended_kin_reshape[:n_batch, :, VEL_DECODING_STATES],\n",
    "                                               (-1, N_DECODING_STATES))\n",
    "\n",
    "\n",
    "        clf = linear_model.Lasso(alpha = lasso_alpha, max_iter = 100000)\n",
    "        clf.fit(spike_count_reshape_slice, intended_kin_reshape_slice)\n",
    "\n",
    "        ols = linear_model.LinearRegression()\n",
    "        ols.fit(spike_count_reshape_slice, intended_kin_reshape_slice)\n",
    "\n",
    "        lasso_coefs_accumulate[n_batch,:,:] = clf.coef_\n",
    "\n",
    "        ols_coefs_accumulate[n_batch,:,:] = ols.coef_\n",
    "    \n",
    "    lasso_coefs_all.append(lasso_coefs_accumulate)\n",
    "    ols_coefs_all.append(ols_coefs_accumulate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.980771Z",
     "start_time": "2021-04-16T22:33:47.054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso_coefs_all =  np.array(lasso_coefs_all)\n",
    "ols_coefs_all = np.array(ols_coefs_all)\n",
    "print(f'the alpha scanning has a shape of {lasso_coefs_all.shape}')\n",
    "print(f'the alphas has a shape of {ols_coefs_all.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.981188Z",
     "start_time": "2021-04-16T22:33:47.057Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lasso_coefs_accumulate.shape)\n",
    "print('lets look at the coefficients')\n",
    "\n",
    "figure, axes = plt.subplots(1, len(lasso_alpha_all), figsize = (12, 4))\n",
    "\n",
    "for i,a in enumerate(lasso_alpha_all):\n",
    "    \n",
    "    lasso_coefs_accumulate = np.squeeze(lasso_coefs_all[i, :,:,:])\n",
    "    ols_coefs_accumulate = np.squeeze(ols_coefs_all[i, :,:,:])\n",
    "\n",
    "    axes[i].plot(np.reshape(lasso_coefs_accumulate, (N_BATCHES, -1)))\n",
    "\n",
    "    #plot the ols\n",
    "    axes[i].plot(np.reshape(ols_coefs_accumulate, (N_BATCHES, -1)),\n",
    "            linestyle = 'dotted')\n",
    "    \n",
    "    axes[i].set_title(f'lasso an alpha {lasso_alpha_all[i]} vs. ols ')\n",
    "\n",
    "    axes[i].set_xlabel('Cumulative number batches')\n",
    "    axes[i].set_ylabel('Coefficients')\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a feature selection class\n",
    "\n",
    "this is heavily modeled after sklearn.feature selection stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.981760Z",
     "start_time": "2021-04-16T22:33:47.060Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureTransformer():\n",
    "    SUPPORTED_METHODS = [\n",
    "        'nothing',\n",
    "        'transpose',\n",
    "        'select_rows',\n",
    "        'select_columns'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, proc_meth_arg, *args, **kwargs):\n",
    "        '''\n",
    "        preprocess_methods[a list of tuples]:  \n",
    "        each tuple of has the format of ('method in string', arguments)\n",
    "        '''\n",
    "        #check if the methods are supported\n",
    "        for meth, arg in proc_meth_arg:\n",
    "            if not self.check_meth_supported(meth): \n",
    "                raise Exception(f'the proc method {meth} is not supported')\n",
    "        \n",
    "        #if supported,\n",
    "        self.proc_meth_arg = proc_meth_arg\n",
    "        \n",
    "    def __call__(self, feature_matrix):\n",
    "        return self.preprocess_features(feature_matrix)\n",
    "    \n",
    "    def preprocess_features(self, feature_matrix):\n",
    "        '''\n",
    "        iteratively applies the methods to the features\n",
    "        '''\n",
    "        feature_matrix_temp = np.copy(feature_matrix)\n",
    "        \n",
    "        for meth, arg in self.proc_meth_arg:\n",
    "            \n",
    "            if meth == 'nothing':\n",
    "                pass\n",
    "            elif meth == 'transpose': \n",
    "                feature_matrix_temp = feature_matrix_temp.T\n",
    "            elif meth =='select_rows':\n",
    "                feature_matrix_temp = feature_matrix_temp[arg, :]\n",
    "            elif meth == 'select_columns':\n",
    "                feature_matrix_temp = feature_matrix_temp[:, arg]\n",
    "            else:\n",
    "                raise Exception(f'unsupported method {meth}')\n",
    "                \n",
    "        return feature_matrix_temp\n",
    "\n",
    "    def check_meth_supported(self, proc_meth:str):\n",
    "        return proc_meth in self.SUPPORTED_METHODS\n",
    "    \n",
    "class TransformerBatchToFit(FeatureTransformer):\n",
    "    X_VEL_STATE_IND = 3\n",
    "    Y_VEL_STATE_IND = 5\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        proc_meth_arg = [\n",
    "            ('select_rows', (self.X_VEL_STATE_IND, self.Y_VEL_STATE_IND)),\n",
    "            ('transpose', ())\n",
    "        ]\n",
    "        \n",
    "        super().__init__(proc_meth_arg, *args, **kwargs)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.982262Z",
     "start_time": "2021-04-16T22:33:47.064Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "A = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "     \n",
    "])\n",
    "\n",
    "feature_meth_arg = [\n",
    "    ('transpose', None ),\n",
    "    ('select_rows',0)\n",
    "]\n",
    "\n",
    "feat_trans = FeatureTransformer(feature_meth_arg)\n",
    "\n",
    "print(feat_trans(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.982715Z",
     "start_time": "2021-04-16T22:33:47.067Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeatureSelector():\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        self._init_feature_transformer(*args, **kwargs)\n",
    "        \n",
    "        self.fit_flag =  False\n",
    "        self.measure_ready = False\n",
    "        \n",
    "        #default threshold to be 0\n",
    "        self.current_threshold = 0\n",
    "        self.threshold_ready = False\n",
    "        \n",
    "        #set up tracking weights\n",
    "        self.history_of_weights = list()\n",
    "        \n",
    "        #this is a list that tracks the features selected\n",
    "        self.selected_feature_indices = np.nan\n",
    "        self.selected_feature_log = []\n",
    "        \n",
    "    def __call__(self, feature_matrix, target_matrix):\n",
    "        tranformed_x, transformed_y = self.transform_features(x_array = feature_matrix, \n",
    "                                                              y_array = target_matrix)\n",
    "        self.measure_features(tranformed_x, transformed_y)\n",
    "        return self.get_feature_weights()\n",
    "        \n",
    "    def transform_features(self, x_array = None, y_array = None):\n",
    "        \n",
    "        transformed_results = list()\n",
    "        \n",
    "        if self.transform_x_flag: \n",
    "            transformed_x_array = self.feature_x_transformer(x_array)\n",
    "            transformed_results.append(transformed_x_array)\n",
    "        else:\n",
    "            transformed_results.append(np.copy(x_array))\n",
    "            \n",
    "        if self.transform_y_flag:\n",
    "            transformed_y_array = self.feature_y_transformer(y_array)\n",
    "            transformed_results.append(transformed_y_array)\n",
    "        else:\n",
    "            transformed_results.append(np.copy(y_array))\n",
    "            \n",
    "        return  tuple(transformed_results)\n",
    "    \n",
    "        \n",
    "    def measure_features(self, feature_matrix, target_matrix):\n",
    "        '''\n",
    "        feature_matrix[np.array]: n_time_points by n_features\n",
    "        target_matrix[np.array]: n_time_points by n_target fitting vars\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def get_feature_weights(self):\n",
    "        pass\n",
    "        \n",
    "    def threshold_features(self):\n",
    "        pass\n",
    "    \n",
    "    def get_selected_feature_indices(self):\n",
    "        return self.selected_feature_indices\n",
    "    \n",
    "    \n",
    "    def _init_feature_transformer(self, *args, **kwargs):\n",
    "        #initialize preproc features\n",
    "        self.transform_x_flag = kwargs['transform_x_flag'] if 'transform_x_flag' in kwargs.keys() else False\n",
    "        self.transform_y_flag = kwargs['transform_y_flag'] if 'transform_y_flag' in kwargs.keys() else False\n",
    "\n",
    "        if self.transform_x_flag:\n",
    "            if 'feature_x_transformer' in kwargs.keys():\n",
    "                self.feature_x_transformer = kwargs['feature_x_transformer']\n",
    "            else: \n",
    "                raise Exception(f'{__name__}: feature_x_transformer specified, but feature_transformer not in kwarg.keys')\n",
    "\n",
    "        if self.transform_y_flag:\n",
    "            if 'feature_y_transformer' in kwargs.keys():\n",
    "                self.feature_y_transformer = kwargs['feature_y_transformer']\n",
    "            else: \n",
    "                raise Exception(f'{__name__}: feature_y_transformer specified, but feature_transformer not in kwarg.keys')\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.983110Z",
     "start_time": "2021-04-16T22:33:47.070Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "     \n",
    "])\n",
    "\n",
    "\n",
    "feature_x_meth_arg = [\n",
    "    ('transpose', None ),\n",
    "    ('select_rows',0)\n",
    "]\n",
    "\n",
    "y = np.array([\n",
    "    [1,2,3],\n",
    "    [4,5,6],     \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "feature_y_meth_arg = [\n",
    "    ('transpose', None )\n",
    "]\n",
    "\n",
    "\n",
    "kwargs_feature = dict()\n",
    "kwargs_feature = {\n",
    "    'transform_x_flag':True,\n",
    "    'transform_y_flag':True,\n",
    "    'feature_x_transformer':TransformerBatchToFit(),\n",
    "    'feature_y_transformer':FeatureTransformer(feature_y_meth_arg)\n",
    "}\n",
    "\n",
    "\n",
    "fs_tester = FeatureSelector(**kwargs_feature)\n",
    "#fs_tester.transform_features(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.983613Z",
     "start_time": "2021-04-16T22:33:47.073Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "class LassoFeatureSelector(FeatureSelector):\n",
    "    \n",
    "    DEFAULT_ALPHA = 1\n",
    "    DEFAULT_MAX_ITERATION = 10000\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        #param related to lasso, default to 1\n",
    "        self.current_lasso = kwargs['lasso_alpha'] if 'lasso_alpha' in kwargs.keys() else self.DEFAULT_ALPHA\n",
    "        self.max_iter = kwargs['max_iter'] if 'max_iter' in kwargs.keys() else self.DEFAULT_MAX_ITERATION\n",
    "\n",
    "        self._init_lasso_regression(self.current_lasso, \n",
    "                                   self.max_iter)\n",
    "        \n",
    "        print(f'{__name__}: initialized lasso regression with an alpha of {self.current_lasso} and a max number of iteration of {self.max_iter}')\n",
    "\n",
    "        \n",
    "    def _init_lasso_regression(self, alpha, max_iter):\n",
    "        '''\n",
    "        set up the model\n",
    "        '''\n",
    "        self.lasso_model = Lasso(alpha = alpha,  \n",
    "                                  max_iter= max_iter)\n",
    "        \n",
    "    \n",
    "    def measure_features(self, feature_matrix, target_matrix):\n",
    "        '''\n",
    "        in this case, features are measured by their lasso weights\n",
    "        \n",
    "        feature_matrix[np.array]: n_time_points by n_features\n",
    "        target_matrix[np.array]: n_time_points by n_target fitting vars\n",
    "        '''\n",
    "        \n",
    "        #fitted results to lasso_model._coef\n",
    "        self.lasso_model.fit(feature_matrix, target_matrix)\n",
    "        self.measure_ready = True\n",
    "        \n",
    "        #save to the history of measures\n",
    "        self.history_of_weights.append(self.get_feature_weights())\n",
    "        \n",
    "    def get_feature_weights(self):\n",
    "        if self.measure_ready: \n",
    "            return self.lasso_model.coef_\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    def get_history_of_weights(self):\n",
    "        return np.array(self.history_of_weights)\n",
    "        \n",
    "    def threshold_features(self):\n",
    "        pass\n",
    "    \n",
    "    def get_selected_feature_indices(self):\n",
    "        return self.selected_feature_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.984066Z",
     "start_time": "2021-04-16T22:33:47.076Z"
    }
   },
   "outputs": [],
   "source": [
    "print('test out the feature selector')\n",
    "\n",
    "lasso_fs = LassoFeatureSelector()\n",
    "\n",
    "print(spike_count_reshape_slice.shape)\n",
    "print(intended_kin_reshape_slice.shape)\n",
    "\n",
    "lasso_fs.measure_features(spike_count_reshape_slice,\n",
    "                         intended_kin_reshape_slice)\n",
    "\n",
    "print(f'this is asking questions about how well the spike count fit to the internal states')\n",
    "lasso_fs.get_feature_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## put it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.984460Z",
     "start_time": "2021-04-16T22:33:47.079Z"
    }
   },
   "outputs": [],
   "source": [
    "time_slice_of_interest = 0\n",
    "print(f'time slice of interest {time_slice_of_interest}')\n",
    "\n",
    "spike_count_slice = np.squeeze(spike_counts_batch[time_slice_of_interest, :, :])\n",
    "intended_kin_slice = np.squeeze(intended_kin[time_slice_of_interest, :, :])\n",
    "kf_C_slice = np.squeeze(kf_C[time_slice_of_interest, :,VEL_DECODING_STATES])\n",
    "\n",
    "print(spike_count_slice.shape)\n",
    "print(intended_kin_slice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.984849Z",
     "start_time": "2021-04-16T22:33:47.082Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_x_meth_arg = [\n",
    "    ('transpose', None ),\n",
    "]\n",
    "\n",
    "\n",
    "kwargs_feature = dict()\n",
    "kwargs_feature = {\n",
    "    'transform_x_flag':True,\n",
    "    'transform_y_flag':True,\n",
    "    'feature_x_transformer':FeatureTransformer(feature_x_meth_arg),\n",
    "    'feature_y_transformer':TransformerBatchToFit()\n",
    "}\n",
    "\n",
    "fs_tester = LassoFeatureSelector(**kwargs_feature)\n",
    "tx_x, tx_y = fs_tester.transform_features(spike_count_slice, intended_kin_slice)\n",
    "\n",
    "print(tx_x.shape)\n",
    "print(tx_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.985369Z",
     "start_time": "2021-04-16T22:33:47.085Z"
    }
   },
   "outputs": [],
   "source": [
    "fs_tester.measure_features(tx_x, tx_y)\n",
    "print(fs_tester.get_feature_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T22:33:47.985844Z",
     "start_time": "2021-04-16T22:33:47.088Z"
    }
   },
   "outputs": [],
   "source": [
    "fs_tester.get_history_of_weights().shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "600.8px",
    "left": "35px",
    "top": "353.8px",
    "width": "293.1px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
